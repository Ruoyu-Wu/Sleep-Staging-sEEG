{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buz_AOAEbprf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import PatchTSTConfig, PatchTSTForClassification, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvuer83Ba399",
        "outputId": "702e5ca6-85ca-4932-bee2-74b6eb22e7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# connect colab notebook to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = np.load('/content/drive/MyDrive/ALL/Research_NS/Sleep/data/MNI_sEEG/train_data_noCoordinates.npz')\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "y = torch.LongTensor(y)\n",
        "\n",
        "regions = data['region']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_regions = label_encoder.fit_transform(regions)\n",
        "X_with_regions = np.column_stack((encoded_regions, X))\n",
        "# Assuming X_with_regions is a numpy array\n",
        "X_with_regions = torch.FloatTensor(X_with_regions)"
      ],
      "metadata": {
        "id": "qgVt4s-DcJy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_with_regions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PtQW-2ofDXu",
        "outputId": "033845d8-2b06-4941-bf8c-9f84ee5fb987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4576, 6801)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "w5T8V_SygjVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_with_regions, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tO90NjYZgs8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sk9wD_2fBFpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None,\n",
        "                 *, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(planes, planes, 1)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.se = SELayer(planes, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    # for older versions of PyTorch.  For new versions you can use nn.GELU() instead.\n",
        "    def __init__(self):\n",
        "        super(GELU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.gelu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MRCNN(nn.Module):\n",
        "    def __init__(self, afr_reduced_cnn_size):\n",
        "        super(MRCNN, self).__init__()\n",
        "        drate = 0.5\n",
        "        self.GELU = GELU()  # for older versions of PyTorch.  For new versions use nn.GELU() instead.\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=50, stride=6, bias=False, padding=24),\n",
        "            nn.BatchNorm1d(64),\n",
        "            self.GELU,\n",
        "            nn.MaxPool1d(kernel_size=8, stride=2, padding=4),\n",
        "            nn.Dropout(drate),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=8, stride=1, bias=False, padding=4),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.Conv1d(128, 128, kernel_size=8, stride=1, bias=False, padding=4),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4, padding=2)\n",
        "        )\n",
        "\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=400, stride=50, bias=False, padding=200),\n",
        "            nn.BatchNorm1d(64),\n",
        "            self.GELU,\n",
        "            nn.MaxPool1d(kernel_size=4, stride=2, padding=2),\n",
        "            nn.Dropout(drate),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.Conv1d(128, 128, kernel_size=7, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(drate)\n",
        "        self.inplanes = 128\n",
        "        self.AFR = self._make_layer(SEBasicBlock, afr_reduced_cnn_size, 1)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):  # makes residual SE block\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.features1(x)\n",
        "        x2 = self.features2(x)\n",
        "        x_concat = torch.cat((x1, x2), dim=2)\n",
        "        x_concat = self.dropout(x_concat)\n",
        "        x_concat = self.AFR(x_concat)\n",
        "        return x_concat\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "\n",
        "def attention(query, key, value, dropout=None):\n",
        "    \"Implementation of Scaled dot product attention\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class CausalConv1d(torch.nn.Conv1d):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 dilation=1,\n",
        "                 groups=1,\n",
        "                 bias=True):\n",
        "        self.__padding = (kernel_size - 1) * dilation\n",
        "\n",
        "        super(CausalConv1d, self).__init__(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=self.__padding,\n",
        "            dilation=dilation,\n",
        "            groups=groups,\n",
        "            bias=bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        result = super(CausalConv1d, self).forward(input)\n",
        "        if self.__padding != 0:\n",
        "            return result[:, :, :-self.__padding]\n",
        "        return result\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, afr_reduced_cnn_size, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "\n",
        "        self.convs = clones(CausalConv1d(afr_reduced_cnn_size, afr_reduced_cnn_size, kernel_size=7, stride=1), 3)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        \"Implements Multi-head attention\"\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        query = query.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key   = self.convs[1](key).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.convs[2](value).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attn = attention(query, key, value, dropout=self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(nbatches, -1, self.h * self.d_k)\n",
        "\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layer normalization module.\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerOutput(nn.Module):\n",
        "    '''\n",
        "    A residual connection followed by a layer norm.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerOutput, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class TCE(nn.Module):\n",
        "    '''\n",
        "    Transformer Encoder\n",
        "\n",
        "    It is a stack of N layers.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(TCE, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    '''\n",
        "    An encoder layer\n",
        "\n",
        "    Made up of self-attention and a feed forward layer.\n",
        "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
        "    '''\n",
        "    def __init__(self, size, self_attn, feed_forward, afr_reduced_cnn_size, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)\n",
        "        self.size = size\n",
        "        self.conv = CausalConv1d(afr_reduced_cnn_size, afr_reduced_cnn_size, kernel_size=7, stride=1, dilation=1)\n",
        "\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        \"Transformer Encoder\"\n",
        "        query = self.conv(x_in)\n",
        "        x = self.sublayer_output[0](query, lambda x: self.self_attn(query, x_in, x_in))  # Encoder self-attention\n",
        "        return self.sublayer_output[1](x, self.feed_forward)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Positionwise feed-forward network.\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Implements FFN equation.\"\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "\n",
        "class AttnSleep(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttnSleep, self).__init__()\n",
        "\n",
        "        N = 2  # number of TCE clones\n",
        "        d_model = 80  # set to be 100 for SHHS dataset\n",
        "        d_ff = 120   # dimension of feed forward\n",
        "        h = 5  # number of attention heads\n",
        "        dropout = 0.1\n",
        "        num_classes = 5\n",
        "        afr_reduced_cnn_size = 30\n",
        "\n",
        "        self.mrcnn = MRCNN(afr_reduced_cnn_size) # use MRCNN_SHHS for SHHS dataset\n",
        "\n",
        "        attn = MultiHeadedAttention(h, d_model, afr_reduced_cnn_size)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.tce = TCE(EncoderLayer(d_model, deepcopy(attn), deepcopy(ff), afr_reduced_cnn_size, dropout), N)\n",
        "\n",
        "        self.fc = nn.Linear(d_model * afr_reduced_cnn_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_feat = self.mrcnn(x)\n",
        "        encoded_features = self.tce(x_feat)\n",
        "        encoded_features = encoded_features.contiguous().view(encoded_features.shape[0], -1)\n",
        "        final_output = self.fc(encoded_features)\n",
        "        return final_output\n",
        "\n",
        "######################################################################\n",
        "\n",
        "class MRCNN_SHHS(nn.Module):\n",
        "    def __init__(self, afr_reduced_cnn_size):\n",
        "        super(MRCNN_SHHS, self).__init__()\n",
        "        drate = 0.5\n",
        "        self.GELU = GELU()  # for older versions of PyTorch.  For new versions use nn.GELU() instead.\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=50, stride=6, bias=False, padding=24),\n",
        "            nn.BatchNorm1d(64),\n",
        "            self.GELU,\n",
        "            nn.MaxPool1d(kernel_size=8, stride=2, padding=4),\n",
        "            nn.Dropout(drate),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=8, stride=1, bias=False, padding=4),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.Conv1d(128, 128, kernel_size=8, stride=1, bias=False, padding=4),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4, padding=2)\n",
        "        )\n",
        "\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=400, stride=50, bias=False, padding=200),\n",
        "            nn.BatchNorm1d(64),\n",
        "            self.GELU,\n",
        "            nn.MaxPool1d(kernel_size=4, stride=2, padding=2),\n",
        "            nn.Dropout(drate),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=6, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.Conv1d(128, 128, kernel_size=6, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            self.GELU,\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(drate)\n",
        "        self.inplanes = 128\n",
        "        self.AFR = self._make_layer(SEBasicBlock, afr_reduced_cnn_size, 1)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):  # makes residual SE block\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.features1(x)\n",
        "        x2 = self.features2(x)\n",
        "        x_concat = torch.cat((x1, x2), dim=2)\n",
        "        x_concat = self.dropout(x_concat)\n",
        "        x_concat = self.AFR(x_concat)\n",
        "        return x_concat"
      ],
      "metadata": {
        "id": "LUe87aFk5q0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AS = AttnSleep()\n",
        "summary(AS, input_size=(64,1,3000))"
      ],
      "metadata": {
        "id": "LALm29v8BXwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}